<div align="center">
<img width="1200" height="475" alt="GHBanner" src="https://github.com/user-attachments/assets/0aa67016-6eaf-458a-adb2-6e31a0763ed6" />
</div>

# Run and deploy your AI Studio app

This contains everything you need to run your app locally.

View your app in AI Studio: https://ai.studio/apps/drive/1LU0H7WJrzGpVFUX0ljk_jv_NbKPYL9cE

• Team hub PRD: See docs/PRD.md for the end-to-end CRM workflow, data model, and API surface.

## Run Locally

**Prerequisites:**  Node.js

1. Install dependencies:
   `npm install`
2. Set the `GEMINI_API_KEY` in [.env.local](.env.local) to your Gemini API key
3. Run the app:
   `npm run dev`

## File Share (Framo G:\\) Integration

The backend can read from a corporate file share (e.g., G:\\ on Windows servers). Configure one of these env vars for the backend:

- `FILE_SHARE_ROOT` absolute path to the share root (preferred)
- `G_DRIVE_PATH` alternative variable name for compatibility

If unset, development defaults to `./backend/files` on non-Windows and `G:\\` on Windows. Endpoints:

- `GET /api/files?path=relative/subdir` list directory contents (auth required)
- `GET /api/file?path=relative/file.txt` read small text files (<=2MB)

Security: paths are resolved safely under the configured root; only read operations are supported by default.

Note: The previously prototyped Excel import feature has been rolled back and is not available in this build.

## Deploy to Render (Cloud)

Option A — Blueprint (recommended)

1. Push to GitHub (done).
2. In Render, choose New > Blueprint and point to `render.yaml` in this repo.
3. Create the services when prompted:
   - Backend (Node) at `/backend`
   - Frontend (Static) at repo root
4. Add a PostgreSQL instance and set `DATABASE_URL` on the backend service (internal DB URL preferred). 
5. Environment variables (backend):
   - `JWT_SECRET` — any strong random string (or auto-generated by Render from blueprint)
   - `ALLOW_ONRENDER` — `true` (enables *.onrender.com CORS)
   - `FILES_ENABLED` — `false` (default)
6. Deploy. Backend runs DB migrations automatically via `npm run migrate` before starting.
7. Frontend receives `VITE_API_URL` from the backend service URL automatically (blueprint wiring).

Option B — Manual services

1. Create a Web Service for `/backend` with:
   - Build Command: `npm install`
   - Start Command: `npm run migrate && node index.js`
   - Env Vars: `DATABASE_URL`, `JWT_SECRET`, `ALLOW_ONRENDER=true`, `FILES_ENABLED=false`
2. Create a Static Site for the frontend (root):
   - Build Command: `npm install && npm run build`
   - Publish Directory: `dist`
   - Env Var: `VITE_API_URL` = the backend service URL

Verification

- Backend health: `GET /api/contacts`
- Migration check: `GET /api/diagnostics/project-type-counts`
# Force rebuild Sun Aug 24 16:37:41 CEST 2025

## Blue/Green DB Staging and ETL (safe rollout)

Use a green database to validate new schema and features without touching production until ready. The backend ships helper scripts.

Prereqs

- `TARGET_DATABASE_URL` (new/green). If `SOURCE_DATABASE_URL` is not set, the ETL uses `DATABASE_URL` from `backend/.env` as the source automatically.

Steps (Option A: separate DB)

1) Apply migrations on green

- In `/backend`, set `DATABASE_URL` to the green DB (export or in `backend/.env`) and run:
   - npm run migrate

2) Clone core data (read-only from live → green)

- In `/backend`, run:
   - TARGET_DATABASE_URL=postgres://… npm run etl:clone
   - (optional) Override source: SOURCE_DATABASE_URL=postgres://… TARGET_DATABASE_URL=postgres://… npm run etl:clone

3) Seed atomic document numbers (optional)

- In `/backend`, run:
   - DATABASE_URL=postgres://… npm run seed:docnums

4) Point a staging backend at green and verify

- Set backend `.env` DATABASE_URL to green. Start backend and frontend. Validate lead conversion (OPP/PRJ numbers), estimator save, and dashboard reports.

5) Cutover

- Update production backend DATABASE_URL to green. Keep old DB as rollback for a window.

Notes

- ETL inserts only when the target table is empty; it won’t overwrite.
- Number allocation is transactional (table `doc_numbers`) with a unique index on `projects.opportunity_number`.

### Option B: separate schema (no new DB required)

If your provider limits the number of databases, you can use a separate schema in the same Postgres instance to stage green changes.

1) Create schema and run migrations into it

- In `/backend`, run migrations with a schema override:
   - DB_SCHEMA=green npm run migrate

2) Point a staging backend at the same DB but with the green schema

- Backend env:
   - DATABASE_URL = your existing Postgres URL
   - DB_SCHEMA = green

3) Validate end-to-end (the staging backend will see the green tables via search_path)

4) Cutover by switching the production backend to DB_SCHEMA=green

Notes

- This avoids creating a new DB; you can switch schemas quickly and roll back by resetting DB_SCHEMA=public.

Schema-to-schema ETL (optional)

- Copy existing data from public → green within the same DB:
   - In `/backend` (uses DATABASE_URL):
      - SOURCE_SCHEMA=public TARGET_SCHEMA=green npm run etl:schema

   Cleanup: remove old schema (after successful cutover)

   - Preconditions:
      - Production backend is running with DB_SCHEMA=green
      - No active connections are using the old schema
      - Backups retained or snapshot taken
   - Then connect with psql (or your DB console) and run:
      - DROP SCHEMA public CASCADE;
      - CREATE SCHEMA public; -- optional, if you want to keep a clean empty public
      - GRANT USAGE ON SCHEMA public TO your_db_user; -- if needed
      - Note: If you prefer to keep public, you can instead drop individual old tables.
